### Coursera Getting and Cleaning Data Project, March 2015

#### Introduction

This README documents the project for Coursera's *Getting and Cleaning Data*
course, March 2015, and the steps taken to meet the project requirements.

The purpose of the project is to demonstrate our ability to collect, work with and
clean a dataset.  The goal of the project is to prepare tidy data for use in
later analysis.  Tidy data can be defined as having a "specific structure, wherein
each variable is a column, each observation is a row, and each type of
observational unit is a table." See [Wikham][3].  This is a peer reviewed and graded
project.

#### Required Submissions

We are required to submit the following:

- a tidy dataset submitted to the coursera course site
- a link to a Github repository containing an R script (run_analysis.R) for performing
the analysis, a code book (CodeBook.md) in markdown format which describes the variables,
the data and any transformations/work performed to clean the data, and this README.

The R script should perform the following operations:

- Merge the training and test datasets to create one dataset
- Extract only the measurements on the mean and standard deviation of each measurements
- Uses descriptive activity names to label the activities in the dataset
- Appropriately labels the dataset with descriptive variable names
- From the dataset in the above step, creates a second, independent tidy dataset with
the average of each variable for each activity and each subject.

#### Data Source and Getting the Data

The work of generating, pre-processing and compiling the data was done by a number of researchers at
[SmartLab][4], a Research Laboratory at the DITEN / DIBRIS Departments of the
University of Genova. The paper describing this work can be found [here][5].  
The data, titled *Human Activity Recognition Using Smartphones
DataSet,*  was made available to us [here][1] by the [UC Irvine Machine Learning Repository][2]

#### Data Description

According to the documentation provided with the dataset, the data was generated and built by
recording 30 volunteer participants (subjects) performing daily living activities while carrying
a waist-mounted smartphone embedded with inertial sensors. The daily activities were:
WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, and LAYING.  The embedded
accelerometer and gyroscope sensors captured 3-axial linear acceleration and 3-axial angular
velocity readings at a constant rate of 50Hz. The resulting dataset was randomly partitioned into
two sets, with 70% of the subjects selected for generating the *"training"* data and 30% the *"test"*
data.  One of our tasks was to combine these two sets into one dataset.

The data pre-processing methodology and variable calculation was described as follows: 

>The sensor signals (accelerometer and gyroscope) were pre-processed by applying noise
>filters and then sampled in fixed-width sliding windows of 2.56 sec and 50% overlap
>(128 readings/window). The sensor acceleration signal, which has gravitational and body
>motion components, was separated using a Butterworth low-pass filter into body acceleration
>and gravity. The gravitational force is assumed to have only low frequency components,
>therefore a filter with 0.3 Hz cutoff frequency was used. From each window, a vector of
>features was obtained by calculating variables from the time and frequency domain.

The **"raw"** data (which we were not required to operate on and clean) for each of the
training and test sets, was provided in in three parts:

- the total tri-axial acceleration (as measured by the accelerometer)
- the estimated body acceleration (generated by low-pass filtering out the acceleration due to gravity)
- the tri-axial angular velocity (as measured by the gyroscope)

This raw data is located in the "Inertial Signals" folder for each of the training and test sets.

As mentioned in the block quote above, these signals where then pre-processed
and used for calculating the 561 variables in the datasets we were required to "clean."

The data we were required to work on was primarily contained in two text files: **X_train.txt** and
**X_test.txt**, for the training and test subjects respectively.  The first, having 70% of the data,
has 7,352 observations of 561 variables (7,352 x 561 table) and the second has 2,947 observations of
the same 561 variables (2,947 x 561 table).  Two accompanying files, **subject_train.txt** and
**subject_test.txt**, identified the subject (participant) for each observation.  The first file is
(7, 352 x 1) and the second is (2,947 x 1).  The subjects are identified by a number from 1 to 30.
Two additional files, **y_train.txt** and **y_test.txt** identified the activity the subjects were
engaged in for each observation.  These files are also (7, 352 x 1) and (2,947 x 1), respectively.
Since there were six activities (WALKING, WALKING_UPSTAIRS, etc.) these files contained values between
1 and 6. Finally, an **activity_labels.txt** file was provided that maps the values (1 to 6) in the
previous two files just mentioned, to the respective character strings describing each activity
(WALKING, WALKING_UPSTAIRS, etc.).  This file is a 6 x 2 table.

#### Data Manipulation

##### Merging the Data

The R script uses the *dplyr* package to operate on the data.  The **X_train.txt** and **X_test.txt** files
were read into a data.frame object using *read.table()*.  The two data frames were then "merged"
together using *bind_rows()* from the dplyr package.  (rbind() would have also worked.) This resulted
in a 10299 x 561 data frame object. This met the first of our five requirements.

##### Selecting the Columns

We were then asked to subset this data frame object, extracting "only the measurements on the mean
and standard deviation of each measurement."  Looking at the **features.txt** file, which lists the
names of the 561 variables showed quite a number of variables with *mean()* and *std()* in their names.
Reading this file into R using *readLines()* and greping for occurrences of *mean* and *std* yielded 79
variable names.  So I went back to thinking what the phrase "of each measurement" might mean.  According
to the README file that came with the data, and as described above, only two things were "measured" by the
phone sensors: tri-axial acceleration by the accelerometer (which was subsequently decomposed into the body
and gravity components via a low-pass filter), and tri-axial angular velocity by the gyroscope.  All the
other variables, according to the **features-info.txt** file, were generated from these three via some kind
of mathematical transformation.  For example, the *Jerk* signals (tBodyAccJerk-XYZ and tBodyGyroJerk-XYZ)
were obtained by taking the time derivative of the body linear acceleration and angular velocity, the *Mag*
family of signals were calculated using the Euclidean norm, and the many frequency domain variables were
calculated by taking the Fast Fourier Transform of their time domain counterparts.  So I would argue that
only three sets of variables constitute measurements and need be selected:  the tri-axial body acceleration
mean and std (columns 1:6), the tri-axial gravity acceleration mean and std (columns 41:46), and the
body gyroscope mean and std (columns 121:126).  Which is what the R script does, by subsetting the columns
as noted.

##### Descriptive Activity Names

We were asked to "use descriptive activity names to label the activities in the dataset".  This implies
adding another column to our data.frame object with the information contained in the **y_train.txt** and
**y_test.txt** files. The files were read by opening a file connection and using *readLines()*.  But like
our numerical data these files were split corresponding to the training and test split of the subjects.
So after reading the vectors had to be concatenated in the same order as the data frames.  At this point
the vector has values in the 1-6 range.  Not very descriptive.  To make it more so, the
**activity_labels.txt** file can be used as a "lookup table" to substitute the numerical values for the
corresponding characters strings (WALKING, etc.). After reading in the **activity_labels.txt** file as a
data frame, a simple function returning the string value for the corresponding numerical value was passed
to *sapply()*, along with the 10,299-long vector of numerical values for each activity.  This gave the
needed vector with which to make a descriptive activity column.  This was done by a call to *bind_cols().*
The column was added to the "front" of the data frame and given the name "activity."

##### Descriptive Variable Names

This step was done prior to adding the activity column as described above.  Up to this point the data frame
columns carried the default V1, V2, etc., as no column header was supplied with the data set.  To generate
more descriptive names programmatically the supplied **features.txt** file was read and used.  This file
gave a name for each variable of the form *1 tBodyAcc-mean()-X.* Using *gsub()* the leading number, space
and "t" were stripped.  In the same way the *()* were removed, and the dashes were replaced with
underscores.  This resulted in names like *BodyAcc_mean_X.* To these was a leading "mean_" was attached
using *paste0,* to get names of the form *mean_BodyAcc_mean_X.* This might seem weird at first, but it
really is a mean of mean values.

Speaking of descriptive variable names, at this point another column was added to the front of the data
frame, to identify the subject (participant).  This information was supplied to us in the form of two
files, **y_train.txt** and **y_test.txt**, as mentioned above.  For each observation in the dataset these
files contain a number between 1-30 identifying the subject.  The files were read using *readLines()*,
concatenated, and the column added to the front of the data frame using *bind_cols()* with the name
"subjects".

##### Tidy Dataset

The required tidy dataset, with "the average for each variable for each activity and each subject," was
generated by two dplyr function calls chained together:  a call to *group_by(subject, activity)* and
the output of that piped to a call to *summarise_each(funs(mean)).*  This resulted in a 180 x 20 table
with six observations for each subject (one for each activity) and the average of the numerical data
variables.

[3]: http://www.jstatsoft.org/v59/i10/paper
[1]: http://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones
[2]: http://archive.ics.uci.edu/ml/index.html
[4]: https://sites.google.com/site/smartlabdibrisunige/
[5]: http://link.springer.com/chapter/10.1007/978-3-642-35395-6_30